\documentclass{article}
\usepackage{times}
\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{algpseudocode}
\usepackage{pdfpages} %insert pdf pages
\usepackage{enumerate}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage[]{algpseudocode}


%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\  }
\rhead{\hmwkTitle}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}


%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Project Report \#3}
\newcommand{\hmwkDueDate}{Apr 8, 2015}
\newcommand{\hmwkClass}{ComS 535x}
\newcommand{\hmwkClassInstructor}{Instructor: Professor Aduri, Pavankumar}
\newcommand{\hmwkAuthorName}{Yuanyuan Tang, Chenguang He}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\\\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ }\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor}}
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

\renewcommand{\thesection}{\arabic{section}}% Remove section references...
\renewcommand{\thesubsection}{\arabic{subsection}}%... from subsections



\renewcommand{\thesubsection}{\arabic{subsection}}
\makeatletter
\def\@seccntformat#1{\@ifundefined{#1@cntformat}%
   {\csname the#1\endcsname\quad}%       default
   {\csname #1@cntformat\endcsname}}%    enable individual control
\newcommand\section@cntformat{}
\makeatother

\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage


\section{WikiCrawler}
\subsection{High level description/pseudo code of your crawling algorithm. In your pseudo code you can use sentences such as "request page from server", "if the page has key words".  Ignore robots.txt and handling network errors in pseudo code}

\begin{algorithm}
\caption{WikiCrawler}
\begin{algorithmic}[1]
\Procedure{WikiCrawler}{$seedUrl, keywords$}
\State trash $\gets$ $\emptyset$
\State visited $\gets$ $\emptyset$
\State robots $\gets$ all disallow url in seedUrl's robots.txt files.
\State queue $\gets$ $\emptyset$
\State max $\gets$ 0
\State numOfEdges $\gets$ 0
\State RequestToWiki $\gets$ 0
\State queue $\gets$ download(seedUrl)
\While{queue is not empty and visited size is less than max}
\State p $\gets$ queue.poll()
\State subs $gets$ all pages that match the restrictions and are not in robot and contains all keywords.
\For{\textbf{each} page in subs}
\If{page is not in $visited$ and $visited$ size $<=$ max}
\State $visited$  add page
\State $queue$ add page
\EndIf
\EndFor
\EndWhile
\For{\textbf{each} page in $visited$}
\State extract links relations in page.
\State print relations.
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}
\newpage

\subsection{"Data structures that you used to maintain "visited" and Q in the crawling algorithm."}
I use hashset to maintains visited and use queue to maintains Q because :

\begin{enumerate}
  \item both visited and Q are NOT needed to maintain elements in order. Therefore Hashset is the fastest way to get element.
  \item In order to implement hash, i design the class Page with hashcode and equal method
  \item Because we do more accesses than insertion, we use hashset to get the best performance.
  \item Because the nodes we need to test are relatively small, hashset is better than BloomFilter which result in over design.
  \item Using Queue to maintains Q is relatively obvious choice.
\end{enumerate}

\newpage

\section{PageRank}
\subsection{Data structure you used to represent the graph.}
I used $HashMap<String, HashSet<String>>$ to represent the graph. The name of each vertex was stored as key value in the HashMap. The name of all the vertices which was connected from the vertex was stored in a $HashSet<String>$. In this way, the graph was represent in the form of adjacency list.

\subsection{Pseudo code for the page rank algorithm}

\begin{algorithm}
\caption{calculate Page Rank(graph, epsilon)}
\begin{algorithmic}[1]
\Procedure{calculate Page Rank}{$graph, \epsilon$}
\State Initialize a vector P$_0$
\State Initialize a vector P$_1$
\State Set P$_0$ to the uniform probability vector [$\frac{1}{N}, ...,  \frac{1}{N}$].
\State P$_1$ $\gets$ P$_0$ 
\State n$\gets$ 0
\State converged $\gets$ false
\While{not converged }
\State compute Rank1 $(graph, P_0)$.
\If{$Norm(P_1, P_0) \leq 0$}
\State converged $\gets$ true
\EndIf
\State n++;
\State $P_0$ $\gets$ $P_1$
\EndWhile
\State return $P_1$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{compute Rank1 from Rank0}
\begin{algorithmic}[1]
\Procedure{compute Rank1}{$graph, Rank_0$}
\State Set P$_1$ to the uniform probability vector [$\frac{1-\beta}{N}, ...,  \frac{1-\beta}{N}$]
\State N $\gets$ number of vertices in the graph
\For{p $\gets$ every page in graph}
\State Q $\gets$ all the pages that are linked from p
\If{$|Q|$ $\neq$ 0}
\For{q $\gets$ all pages in Q}
\State $Rank1_1(q) = Rank1_1(q)$ + $\beta\frac{Rank_0(p)}{|Q|}$
\EndFor
\EndIf
\If{ $|Q|$ = 0}
\For{q $\gets$ all pages in Q}
\State $Rank1_1(q) = Rank1_1(q)$ + $\beta\frac{Rank_0(p)}{N}$
\EndFor
\EndIf
\EndFor

\State return $Rank_1$
\EndProcedure
\end{algorithmic}
\end{algorithm}
\clearpage

\section{WikiTennisCrawler}
\subsection{Time taken by your crawler, Number of nodes and edges in the graph constructed.}
Number of Requst to Wiki: 389 requests
Numberof Edges added: 2744 Edges
Time used:82.514 seconds (including sleeping)

\clearpage

\section{MyWikiCrawler}
\subsection{Root url, Time taken, number of nodes and edge in the graph}

Number of Requst to Wiki: 389 requests
Numberof Edges added: 2744 Edges
Time used:82.514 seconds (including sleeping)

\clearpage

\section{WikiTennisRanker}
The number of vertices in the graph is 100\\
When $\epsilon$ = 0.1\\

The number of iteration at the epsilon = 0.1 is 3\\
The highest rank page is /wiki/Grand\_Slam\_(tennis).\\
The highest in degree page is /wiki/Grand\_Slam\_(tennis).\\
The highest out degree page is /wiki/tennis.\\
The similarity between top100pagerank and top100inDegree is 1.0.\\
The similarity between top100pagerank and top100outDegree is 1.0.\\
The similarity between top100inDegree and top100outDegree is 1.0.\\

When $\epsilon$ = 0.5\\
The number of iteration at the epsilon = 0.5 is 1\\
The highest rank page is /wiki/Grand\_Slam\_(tennis)\\
The highest in degree page is /wiki/Grand\_Slam\_(tennis)\\
The highest out degree page is /wiki/tennis\\
The similarity between top100pagerank and top100inDegree is 1.0\\
The similarity between top100pagerank and top100outDegree is 1.0\\
The similarity between top100inDegree and top100outDegree is 1.0\\

Since the total number of vertices in this graph is 100, top100pagerank, top100inDegree and top100outDegree contain the same terms. The Jaccard similarity of any two sets would be 1. We also found that the larger the $\epsilon$ be, the fewer iterations we have.
\section{MyWikiRanker}

the number of vertices in the graph is 1000 \\
When $\epsilon$ = 0.1\\
The number of iteration at the epsilon = 0.1 is 3 \\

The highest rank page is /wiki/Basketball\\
The highest in-degree page is /wiki/Basketball\\
The highest out-degree page is /wiki/Outline\_of\_basketball\\
The similarity between top100pagerank and top100inDegree is 0.48148148148148145\\
The similarity between top100pagerank and top100outDegree is 0.1834319526627219\\
The similarity between top100inDegree and top100outDegree is 0.41843971631205673\\

When $\epsilon$ = 0.5\\
The number of iteration at the epsilon = 0.5 is 1 \\
The highest rank page is /wiki/Basketball \\
The highest in-degree page is /wiki/Basketball \\
The highest out-degree page is /wiki/Outline\_of\_basketball \\
The similarity between top100pagerank and top100inDegree is 0.41843971631205673 \\
The similarity between top100pagerank and top100outDegree is 0.16279069767441862 \\
The similarity between top100inDegree and top100outDegree is 0.41843971631205673 \\

We found that Jaccard similarity of top100inDegree and top100outDegree was the same no matter what the value of $\epsilon$ we chose. We also found that from Jaccard similarity, top100pagerank was much more similar with top100inDegree than with top100outDegree. This is consistent with the definition of page rank: the more pages link to a certain page, the higher the Page Rank of this page
\end{document} 