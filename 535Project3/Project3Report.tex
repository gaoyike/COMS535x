\documentclass{article}
\usepackage{times}
\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{algpseudocode}
\usepackage{pdfpages} %insert pdf pages
\usepackage{enumerate}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage[]{algpseudocode}


%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\  }
\rhead{\hmwkTitle}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}


%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Project Report \#3}
\newcommand{\hmwkDueDate}{Apr 8, 2015}
\newcommand{\hmwkClass}{ComS 535x}
\newcommand{\hmwkClassInstructor}{Instructor: Professor Aduri, Pavankumar}
\newcommand{\hmwkAuthorName}{Yuanyuan Tang, Chenguang He}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\\\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ }\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor}}
    \vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

\renewcommand{\thesection}{\arabic{section}}% Remove section references...
\renewcommand{\thesubsection}{\arabic{subsection}}%... from subsections



\renewcommand{\thesubsection}{\arabic{subsection}}
\makeatletter
\def\@seccntformat#1{\@ifundefined{#1@cntformat}%
   {\csname the#1\endcsname\quad}%       default
   {\csname #1@cntformat\endcsname}}%    enable individual control
\newcommand\section@cntformat{}
\makeatother

\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage


\section{WikiCrawler}
\subsection{High level description/pseudo code of your crawling algorithm. In your pseudo code you can use sentences such as "request page from server", "if the page has key words".  Ignore robots.txt and handling network errors in pseudo code}

\begin{algorithm}
\caption{WikiCrawler}
\begin{algorithmic}[1]
\Procedure{WikiCrawler}{$seedUrl, keywords$}
\State trash $\gets$ $\emptyset$
\State visited $\gets$ $\emptyset$
\State robots $\gets$ all disallow url in seedUrl's robots.txt files.
\State queue $\gets$ $\emptyset$
\State max $\gets$ 0
\State numOfEdges $\gets$ 0
\State RequestToWiki $\gets$ 0
\State queue $\gets$ download(seedUrl)
\While{queue is not empty and visited size is less than max}
\State p $\gets$ queue.poll()
\State subs $gets$ all pages that match the restrictions and are not in robot and contains all keywords. increase $max$ by the visit pages.
\For{\textbf{each} page in subs}
\If{page is not in $visited$ and $visited$ size $<=$ max}
\State $visited$  add page
\State $queue$ add page
\EndIf
\EndFor
\EndWhile
\For{\textbf{each} page in $visited$}
\State extract links relations in page.
\State print relations.
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}
\newpage

\subsection{"Data structures that you used to maintain "visited" and Q in the crawling algorithm."}
I use hashset to maintains visited and use queue to maintains Q because :

\begin{enumerate}
  \item both visited and Q are NOT needed to maintain elements in order. Therefore Hashset is the fastest way to get element.
  \item In order to implement hash, i design the class Page with hashcode and equal method
  \item Because we do more accesses than insertion, we use hashset to get the best performance.
  \item Because the nodes we need to test are relatively small, hashset is better than BloomFilter which result in over design.
  \item Using Queue to maintains Q is relatively obvious choice.
\end{enumerate}

\newpage

\section{PageRank}
\subsection{Data structure you used to represent the graph.}
I used $HashMap<String, HashSet<String>>$ to represent the graph. The name of each vertex was stored as key value in the HashMap. The name of all the vertices which was connected from the vertex was stored in a $HashSet<String>$. In this way, the graph was represent in the form of adjacency list.

\subsection{Pseudo code for the page rank algorithm}

\begin{algorithm}
\caption{calculate Page Rank(graph, epsilon)}
\begin{algorithmic}[1]
\Procedure{calculate Page Rank}{$graph, \epsilon$}
\State Initialize a vector P$_0$
\State Initialize a vector P$_1$
\State Set P$_0$ to the uniform probability vector [$\frac{1}{N}, ...,  \frac{1}{N}$].
\State P$_1$ $\gets$ P$_0$
\State n$\gets$ 0
\State converged $\gets$ false
\While{not converged }
\State compute Rank1 $(graph, P_0)$.
\If{$Norm(P_1, P_0) \leq \epsilon$}
\State converged $\gets$ true
\EndIf
\State n++;
\State $P_0$ $\gets$ $P_1$
\EndWhile
\State return $P_1$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{compute Rank1 from Rank0}
\begin{algorithmic}[1]
\Procedure{compute Rank1}{$graph, Rank_0$}
\State $\beta$ $\gets$ 0.8
\State N $\gets$ number of vertices in the graph
\State Set Rank$_1$ to the uniform probability vector [$\frac{1-\beta}{N}, ...,  \frac{1-\beta}{N}$]

\For{p $\gets$ every page in graph}
\State Q $\gets$ all the pages that are linked from p
\If{$|Q|$ $\neq$ 0}
\For{q $\gets$ all pages in Q}
\State $Rank_1(q) = Rank_1(q)$ + $\beta\frac{Rank_0(p)}{|Q|}$
\EndFor
\EndIf
\If{ $|Q|$ = 0}
\For{q $\gets$ all pages in Q}
\State $Rank_1(q) = Rank_1(q)$ + $\beta\frac{Rank_0(p)}{N}$
\EndFor
\EndIf
\EndFor

\State return $Rank_1$
\EndProcedure
\end{algorithmic}
\end{algorithm}
\clearpage

\section{WikiTennisCrawler}
\subsection{Time taken by your crawler, Number of nodes and edges in the graph constructed.}
Number of Vertices is 1000.\\
Number of Requst to Wiki: 6087 requests\\
Numberof Edges added: 96402 Edges\\
Time used:1940.671 seconds (including sleeping)\\

\clearpage

\section{MyWikiCrawler}
\subsection{Root url, Time taken, number of nodes and edge in the graph}
seed Url: /wiki/basketball\\
Keywords: basketball nba\\
Number of Requst to Wiki: 5763 requests\\
Number of Edges added: 103051 Edges\\
Time used:992.504 seconds (including sleeping)\\

\clearpage

\section{WikiTennisRanker}
The number of vertices in the graph is 1000.\\
when $\epsilon$ = 0.1, \\
The number of iterations is 2.\\
The highest rank page is: /wiki/France\\
The highest in degree page is /wiki/Australia\\
The highest out degree page is /wiki/Rod\_Laver\\
The similarity between top100pagerank and top100inDegree is 0.8018018018018018\\
The similarity between top100pagerank and top100outDegree is 0.42857142857142855\\
The similarity between top100inDegree and top100outDegree is 0.45985401459854014\\
when $\epsilon$ = 0.05, \\
The number of iterations is 3.\\
The highest rank page is: /wiki/France\\
The highest in degree page is /wiki/Australia\\
The highest out degree page is /wiki/Rod\_Laver\\
The similarity between top100pagerank and top100inDegree is 0.7391304347826086\\
The similarity between top100pagerank and top100outDegree is 0.42857142857142855\\
The similarity between top100inDegree and top100outDegree is 0.45985401459854014  \\

Since top100inDegree and top100outDegree contain the same terms no matter what $\epsilon$ we choose, the Jaccard similarity of those two sets would be the same. We also found that the larger the $\epsilon$ be, the fewer iterations we have. We also found that from Jaccard similarity, top100pagerank was much more similar with top100inDegree than with top100outDegree. This is consistent with the definition of page rank: the more pages link to a certain page, the higher the Page Rank of this page.\\
\section{MyWikiRanker}

the number of vertices in the graph is 1000 \\
When $\epsilon$ = 0.1\\
The number of iteration is 3.\\
The highest rank page is /wiki/Basketball\\
The highest in-degree page is /wiki/Basketball\\
The highest out-degree page is /wiki/Outline\_of\_basketball\\
The similarity between top100pagerank and top100inDegree is 0.47058823529411764\\
The similarity between top100pagerank and top100outDegree is 0.17647058823529413\\
The similarity between top100inDegree and top100outDegree is 0.4084507042253521\\

When $\epsilon$ = 0.05\\
The number of iteration is 3.\\
The highest rank page is: /wiki/Basketball\\
The highest in degree page is /wiki/Basketball\\
The highest out degree page is /wiki/Outline\_of\_basketball\\
The similarity between top100pagerank and top100inDegree is 0.47058823529411764\\
The similarity between top100pagerank and top100outDegree is 0.17647058823529413\\
The similarity between top100inDegree and top100outDegree is 0.4084507042253521\\

We found that the highest page rank, in-degree page and out-degree page did not change with $\epsilon$ .
\end{document} 